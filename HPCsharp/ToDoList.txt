// TODO: Bring in parallel Quick Sort from the link that John provided (http://dzmitryhuba.blogspot.com/2012/03/parallel-partition-phase-for-quick-sort.html)
//       Or, port from C++ parallel implementation provided in Pro TBB book from Intel (chapter 2).
// TODO: Figure out why LSD Radix Sort is 3X slower when compiled with "g++ -O2" versus same code (is it?) using VisualStudio 2019 (same optimization switches?), since
//       it is the same code.
// TODO: Optimize small size sorting cases by doing a bunch of sub-array sorting and benchmarking performance. It would be very powerful to show speedup at the lower
//       size end of the spectrum, as that would show usefulness to more users, and may require completely different optimizations and thinking.
// TODO: Add to the Faster Copy blog the finding of Array.Copy() being slower than a for loop to copy an array. This could also be part of a blog on how to write higher performance C# code.
// TODO: Add a blog entry for which language is fastest at multi-core sorting, and possibly include GPUs.
// TODO: Add InitLibrary() call, which will figure out details about the machine we are running on: number of physical and virtual cores, number of memory channels, etc.
//       Otherwise common assumptions will be setup as defaults, which are dangerous long term. Create a namespace for SystemAttributes with variety of values.s
// TODO: Implement Heap Sort, since it's in-place and efficient for medium size arrays. It's use as a termination base case for some divide-and-conquer algorithms
//       should provide a potentially substantial speedup over Insertion Sort, since Heap Sort would terminate at a much larger array size threshold, since its O(nlgn)
//       vs Insertion Sort being O(N^2), which would reduce the number of recursion levels - important especially for in-place sort algorithms.
//       This TODO is a nice-to-have, as .Sort() already implements Heap Sort and QuickSort combination. We should consider using it instead, wherever we use Insertion Sort.
// TODO: Consider making a podcast on HPCsharp and algorithm performance topics, since it's quite a bit simpler to create than videos.
// TODO: Determine the minimum work quanta where doing work by more than one core/worker uses less time than doing the same amount of work by a single core/worker.
//       Also, determine when doing work by more than one core/worker uses less than N-percent (e.g. 5% or less) of overhead - i.e. the cost of parallelism is negligible.
// TODO: Start a document/paper with all of these parallel factors that matter, such as this minimum work quanta, paged in or not of memory, aligned or unaligned scalar
//       and SIMD/SSE size items from memory.
// TODO: Figure out how to have HPCsharp support .AsParallel() construct and .WithDegreeOfParallelism() also. This would make usage much simpler of many HPCsharp functions.
// TODO: Add support for .AsParallel() and WithDegreeOfParallelism(), to simplify the interface for all parallel implementations
// TODO: Add scalar and parallel Select algorithm: http://rosettacode.org/wiki/Quickselect_algorithm#C.23 (for scalar code)
// TODO: Develop algorithms, including hybrid sorting that take advantage of CPU caches well by operating on small enough chunks which fit into cache well and use
//       cache properties well. These operations can then be combined/composed/cascaded well, possibly by specifying how to combine these atomic blocks of operations
//       or interleave these operations to fully utilize computational and cache bandwidth resources the CPU has to offer.
//       This could bring at least 2X speedup, as summation by itself is at least 2X faster in-cache than in-system-memory.
//       This could lead a shift from functional programming to micro-functional programming, where small kernel functions are combined, with a loop
//       over a larger array on the outermost loop - i.e. instead of myArray.functionA().FunctionB().FunctionC() it would be
//       for loop over the array { fewElementsOfArray.FunctionA().FunctionB().FunctionC() }
// TODO: Write a blog about the need to reduce system memory latency: to not leave algorithms behind, to raise performance
// TODO: Write a blog about the need to improve random access time to system memory, as a way to raise performance (not just cache random accesss time)
// TODO: Write a blog about the loss of performance when using SIMD/SSE with several nested if statements, because of the need to execute all of the branches on all of the SIMD data items.
// TODO: Write a blog about how unsafe CRC-32 is for generating a unique value, giving an example of a letter A being white
//       positioned anywhere on a black background, producing the same CRC-32 value
// TODO: Implement DotProduct of two arrays, and SSE kernels/atoms, which can be composed. Kernel/Atom may be way more useful than
//       dot product of a large array. But, a general SSE dot-product of any length would be useful, even if they are small arrays.
// TODO: Add SoftMax algorithm used by Machine Learning: https://stackoverflow.com/questions/42599498/numercially-stable-softmax
// TODO: Implement a control algorithm for the number of cores that participate in the algorithm, to determine the optimal
//       number of cores dynamically at run-time. Start out processing a chunk that is 1X of the work size, measure the execution
//       time, then double the number of cores and the work size, measure the execution time, if faster then increase the number
//       of cores and work size, if slower then decrease the number of cores and the work size. This method free's the developer
//       from experimentally running the algorithm to determine the optimal numnber of cores, and especially useful when
//       other computational resources, such as memory bandwidth or network bandwidth are limiting performance.
// TODO: Add Array equal List and List equal Array versions, as Microsoft supports these variations, since they use IEnumerable. Plus, they can compare other collections potentially, if it makes sense or is possible at all, since they are IEnumerable, but are slower
// TODO: Figure out what Linq.Min returns when an array is made of nullable elements and add support for this type of an array
// TODO: Linq provides different variations of Min and Max than HPC#. Implement similar methods to Linq, without using IEnumerable
// TODO: Add Min and Max to return the index of the minimum and maximum element within the container. Linq does not have these functions. Maybe that's
//       not necessary in C# as a reference to the object is returned instead of a copy of the object
// TODO: Add IEnumerable versions of InsertionSort to provide both levels of abstraction, if possible
// TODO: Add IEnumerable versions of Merge to provide both levels of abstraction, and more merging flexibilities, such as Merge(array, List), if possible
// TODO: Add IEnumerable parallel copy to support higher level of abstraction high performance copy
// TODO: Add parallel array and list copy, and find out other containers that can be be accessed without IEnumerable and parallel copy them too
// TODO: Write a blog about IEnumerable needs support for random access and that random access is a must (at least divide in half) to support parallelism
//       in the form of divide and conquer
// TODO: Learn span and how that can be used to support divide-and-conquer for IEnumerable
// TODO: Explore if support for IList instead of List is possible to support even more generic algorithms. Since IList is 2X slower provide separate implementations
//       and see if C# compiler will not show conflict between List and IList implementations. This provides user a choice of abstraction versus performance
// TODO: Provide merge sort for users, since I could make it stable possibly and not be O(n^2) in the worst case performance, and could possibly make it
//       more generic than my current version
// TODO: Definitely compare my Insertion Sort with Microsoft's on arrays of small random arrays
// TODO: Create a simpler Merge to merge entire Lists
// TODO: Implement MinMax parallel algorithm from my Dr. Dobb's article on optimal object extents, and implement a parallel version of it.
// TODO: Implement Merge Sort with Comparison interface, just like the second interface that C# standard .Sort() has, to support lamdas and the
//       ability to add comparison method (lambda) to either user defined class or a standard class.
// TODO: Since Parallel Merge Sort uses .Sort as the base case, the whole algorithm becomes non-stable => stability can not be claimed.
//       As we determiner DivideAndConquer to be not-stable Parallel Merge Sort isn't stable also
// TODO: Bring back Stable Merge Sort (parallel only, since serial Merge Sort is already stable) and provide parallel version of it. This will provide limited
//       acceleration, but at least will provide some and will beat LINQ, since that's the only current option for C# developers.
// TODO: List all available algorithms in the Readme, right up front to allow new users to see what's available to them
// TODO: Figure out what the deprecated warning during the building of the NuGet package is all about
// TODO: Figure out a way to eliminate if statements (branches) in many common cases for higher performance. This would make a good blog or technical paper.
//       Compare performance gains for modern highly pipelined CPUs. This is something compilers should be doing whenever they can.
// TODO: Change all benchmark results to up to and quote only the maximum performance of one algorithm versus the maximum performance of another we are comparing
//       to. This is more fair and simpler to understand and according to Andrei Alexandrescu it's the right way to measure performance of an algorithm - what
//       is its best possible performance, as all other factors will only add to the time and slow it down as we measure multiple times with variety of interference
//       factors.
// TODO: Implement a faster MergeSort and QuickSort of array of objects/classes based on a field, by pulling the field out
//       along with the orginal index into an array of Sruct's, which will be stored linearly together in memory, instead
//       of being spread out all over memory as array of objects/classes are. This should be a generic/general optimization
//       technique applicable to other algorithms to create a DRAM-system-memory-friendly access pattern (since it stinks at
//       truly random access pattern by 100X worse than sequential access).
// TODO: Study and implement Shear Sort, which is used to sort 2-D meshes and for distrubuted sorting